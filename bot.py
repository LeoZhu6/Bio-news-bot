import os
import re
import html
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus

import requests
import feedparser
from bs4 import BeautifulSoup
import trafilatura


# ========= é…ç½®åŒº =========
COMPANIES = [
    ("Pfizer è¾‰ç‘", ("Pfizer", "è¾‰ç‘")),
    ("Merck é»˜æ²™ä¸œ", ("Merck", "é»˜æ²™ä¸œ", "MSD")),
    ("J&J å¼ºç”Ÿ", ("Johnson & Johnson", "J&J", "å¼ºç”Ÿ")),
    ("Roche ç½—æ°", ("Roche", "ç½—æ°")),
    ("Novartis è¯ºå", ("Novartis", "è¯ºå")),
    ("AstraZeneca é˜¿æ–¯åˆ©åº·", ("AstraZeneca", "é˜¿æ–¯åˆ©åº·")),
    ("GSK è‘›å…°ç´ å²å…‹", ("GSK", "GlaxoSmithKline", "è‘›å…°ç´ å²å…‹")),
    ("Sanofi èµ›è¯ºè²", ("Sanofi", "èµ›è¯ºè²")),
    ("BMS ç™¾æ—¶ç¾æ–½è´µå®", ("Bristol Myers Squibb", "BMS", "ç™¾æ—¶ç¾æ–½è´µå®")),
    ("AbbVie è‰¾ä¼¯ç»´", ("AbbVie", "è‰¾ä¼¯ç»´")),
    ("Amgen å®‰è¿›", ("Amgen", "å®‰è¿›")),
    ("Eli Lilly ç¤¼æ¥", ("Eli Lilly", "Lilly", "ç¤¼æ¥")),
    ("Novo Nordisk è¯ºå’Œè¯ºå¾·", ("Novo Nordisk", "è¯ºå’Œè¯ºå¾·")),
    ("Moderna", ("Moderna",)),
    ("BioNTech", ("BioNTech",)),
    ("æ’ç‘åŒ»è¯", ("æ’ç‘åŒ»è¯", "Hengrui")),
    ("ç™¾æµç¥å·", ("ç™¾æµç¥å·", "BeiGene")),
    ("è¯æ˜åº·å¾·", ("è¯æ˜åº·å¾·", "WuXi AppTec")),
    ("å¤æ˜ŸåŒ»è¯", ("å¤æ˜ŸåŒ»è¯", "Fosun Pharma")),
    ("ä¿¡è¾¾ç”Ÿç‰©", ("ä¿¡è¾¾ç”Ÿç‰©", "Innovent")),
    ("å›å®ç”Ÿç‰©", ("å›å®ç”Ÿç‰©", "Junshi Biosciences")),
]

EXTRA_KEYWORDS_DEFAULT = [
    "FDA", "EMA", "NMPA",
    "clinical trial", "Phase 1", "Phase 2", "Phase 3",
    "acquisition", "merger", "partnership", "licensing",
    "approval", "complete response letter", "CRL",
    "earnings", "guidance"
]

MAX_ITEMS = int(os.getenv("MAX_ITEMS", "10"))
DAYS_LOOKBACK = int(os.getenv("DAYS_LOOKBACK", "2"))

# æ›´é•¿æ‘˜è¦ï¼šé»˜è®¤ 5 æ¡è¦ç‚¹ï¼›æ¯æ¡æœ€é•¿ 420 å­—ç¬¦ï¼ˆè‹±æ–‡/ä¸­æ–‡å­—ç¬¦éƒ½æŒ‰é•¿åº¦ç®—ï¼‰
BULLETS_PER_ITEM = int(os.getenv("BULLETS_PER_ITEM", "5"))
BULLET_MAX_CHARS = int(os.getenv("BULLET_MAX_CHARS", "420"))

# å…è´¹ LibreTranslateï¼šå…¬å…±å®ä¾‹å¯èƒ½é™æµ/ä¸ç¨³ï¼Œæ‰€ä»¥åšè½®è¯¢+å¤±è´¥é™çº§ï¼ˆä¸è®© workflow å´©ï¼‰
# ä½ å¯ä»¥åœ¨ GitHub Actions env é‡Œè®¾ç½® LIBRETRANSLATE_URL ä¸ºä½ æ›´ç¨³å®šçš„å®ä¾‹
LIBRETRANSLATE_URLS = [
    os.getenv("LIBRETRANSLATE_URL", "").strip(),
    "https://libretranslate.de/translate",
    "https://translate.astian.org/translate",
]
LIBRETRANSLATE_URLS = [u for u in LIBRETRANSLATE_URLS if u]


# ========= é€šç”¨å·¥å…· =========
def google_news_rss_url(query: str) -> str:
    q = quote_plus(query)
    return f"https://news.google.com/rss/search?q={q}&hl=en-US&gl=US&ceid=US:en"


def clean_html_to_text(s: str) -> str:
    soup = BeautifulSoup(s or "", "lxml")
    text = soup.get_text(" ", strip=True)
    return re.sub(r"\s+", " ", text).strip()


def parse_entry_time(entry):
    tm = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not tm:
        return None
    try:
        return datetime(*tm[:6], tzinfo=timezone.utc)
    except Exception:
        return None


def esc(x: str) -> str:
    return html.escape(x or "")


def try_get_og_image(url: str, timeout: float = 10.0):
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "lxml")
        for attrs in (
            {"property": "og:image"},
            {"name": "og:image"},
            {"property": "twitter:image"},
            {"name": "twitter:image"},
        ):
            tag = soup.find("meta", attrs=attrs)
            if tag and tag.get("content"):
                img = tag["content"].strip()
                if img.startswith("http"):
                    return img
    except Exception:
        return None
    return None


# ========= ç¿»è¯‘ï¼ˆå…è´¹ï¼‰ =========
def _split_text(text: str, chunk_size: int = 900) -> list[str]:
    text = (text or "").strip()
    if not text:
        return []
    chunks, buf = [], []
    n = 0
    # æŒ‰æ¢è¡Œä¼˜å…ˆåˆ‡ï¼Œå‡å°‘è¯­ä¹‰ç ´ç¢
    for part in re.split(r"(\n+)", text):
        if not part:
            continue
        if n + len(part) > chunk_size and buf:
            chunks.append("".join(buf).strip())
            buf, n = [], 0
        buf.append(part)
        n += len(part)
    if buf:
        chunks.append("".join(buf).strip())
    return chunks


def libre_translate(text: str, source: str = "auto", target: str = "zh") -> str:
    """
    å…è´¹ç¿»è¯‘ï¼šè½®è¯¢å¤šä¸ª LibreTranslate å…¬å…±å®ä¾‹ã€‚
    - source ä½¿ç”¨ autoï¼Œæé«˜æ··åˆè¯­è¨€æ–‡æœ¬çš„æˆåŠŸç‡
    - é‡åˆ°é™æµ/æœåŠ¡é”™è¯¯è‡ªåŠ¨åˆ‡æ¢å®ä¾‹
    - å…¨éƒ¨å¤±è´¥åˆ™è¿”å›åŸæ–‡ï¼ˆä¿è¯ä»»åŠ¡ä¸å¤±è´¥ï¼‰
    """
    text = (text or "").strip()
    if not text:
        return ""

    chunks = _split_text(text, chunk_size=900)
    out_chunks = []

    for ch in chunks:
        translated = None

        for url in LIBRETRANSLATE_URLS:
            try:
                r = requests.post(
                    url,
                    timeout=25,
                    data={
                        "q": ch,
                        "source": source,
                        "target": target,
                        "format": "text",
                    },
                    headers={"User-Agent": "Mozilla/5.0"},
                )

                # å¸¸è§é™æµ/ä¸ç¨³å®šï¼šæ¢ä¸‹ä¸€ä¸ªå®ä¾‹
                if r.status_code in (429, 500, 502, 503, 504):
                    continue

                r.raise_for_status()
                data = r.json()
                translated = (data.get("translatedText") or "").strip()
                if translated:
                    break
            except Exception:
                continue

        out_chunks.append(translated if translated else ch)

    return "\n".join(out_chunks).strip()


# ========= æ­£æ–‡æŠ½å–ä¸æ‘˜è¦ =========
def fetch_article_text(url: str) -> str:
    """å°½é‡æŠ½å–æ–°é—»æ­£æ–‡ï¼›é‡åˆ°ä»˜è´¹å¢™/åçˆ¬ä¼šå¤±è´¥ï¼Œè°ƒç”¨å¤„ä¼šè‡ªåŠ¨é™çº§åˆ° RSS summary."""
    try:
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return ""
        extracted = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=False,
            favor_recall=True,
        )
        return (extracted or "").strip()
    except Exception:
        return ""


def naive_bullets(text: str, max_bullets: int = 5) -> list[str]:
    """
    è½»é‡æ‘˜è¦ï¼šå–å‰è‹¥å¹²å¥ä½œä¸ºè¦ç‚¹ï¼ˆè¶³å¤Ÿç¨³å®šï¼Œä¸ä¾èµ–é¢å¤– APIï¼‰ã€‚
    """
    t = re.sub(r"\s+", " ", (text or "").strip())
    if not t:
        return []

    parts = re.split(r"(?<=[\.\!\?ã€‚ï¼ï¼Ÿ])\s+", t)
    bullets = []
    for p in parts:
        p = p.strip()
        if len(p) < 30:
            continue
        bullets.append(p[:BULLET_MAX_CHARS])
        if len(bullets) >= max_bullets:
            break

    if not bullets:
        bullets = [t[:BULLET_MAX_CHARS]]

    return bullets


# ========= Telegram =========
def tg_send_message(token: str, chat_id: str, text: str):
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    r = requests.post(
        url,
        timeout=20,
        data={
            "chat_id": chat_id,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": "true",
        },
    )
    r.raise_for_status()


def tg_send_photo(token: str, chat_id: str, photo_url: str, caption: str):
    url = f"https://api.telegram.org/bot{token}/sendPhoto"
    r = requests.post(
        url,
        timeout=25,
        data={"chat_id": chat_id, "caption": caption, "parse_mode": "HTML"},
        params={"photo": photo_url},
    )
    # å›¾ç‰‡å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
    return r.status_code < 400


# ========= æŠ“å–æ–°é—» =========
def fetch_news():
    extra_keywords = [x.strip() for x in os.getenv("EXTRA_KEYWORDS", "").split(",") if x.strip()]
    if not extra_keywords:
        extra_keywords = EXTRA_KEYWORDS_DEFAULT

    cutoff = datetime.now(timezone.utc) - timedelta(days=DAYS_LOOKBACK)
    all_items = []
    seen_links = set()

    for company_name, queries in COMPANIES:
        base = "(" + " OR ".join([f'"{q}"' if " " in q else q for q in queries]) + ")"
        extra = "(" + " OR ".join([f'"{k}"' if " " in k else k for k in extra_keywords]) + ")"
        q = f"{base} {extra}"

        feed = feedparser.parse(google_news_rss_url(q))
        for e in feed.entries:
            title = (e.get("title") or "").strip()
            link = (e.get("link") or "").strip()
            if not title or not link or link in seen_links:
                continue

            published = parse_entry_time(e)
            if published and published < cutoff:
                continue

            source = ""
            if e.get("source") and isinstance(e["source"], dict):
                source = (e["source"].get("title") or "").strip()

            rss_summary = clean_html_to_text(e.get("summary", "") or e.get("description", "") or "")

            all_items.append(
                {
                    "company": company_name,
                    "title": title,
                    "link": link,
                    "source": source,
                    "published": published.isoformat() if published else "",
                    "rss_summary": rss_summary,
                }
            )
            seen_links.add(link)

    all_items.sort(key=lambda x: x.get("published") or "", reverse=True)
    return all_items[:MAX_ITEMS]


def build_cn_digest(items: list[dict]) -> tuple[str, list[dict]]:
    lines = []
    lines.append("<b>ğŸ§¬ åŒ»è¯å¤§å‚æ–°é—»é€Ÿé€’ï¼ˆä¸­æ–‡è¦ç‚¹ï¼‰</b>")
    lines.append(f"<i>{esc(datetime.now().strftime('%Y-%m-%d %H:%M'))}</i>")
    lines.append("")

    enriched = []

    for idx, it in enumerate(items, 1):
        # æŠ½æ­£æ–‡ï¼›ä¸è¶³åˆ™ç”¨ RSS summary/æ ‡é¢˜é™çº§
        article_text = fetch_article_text(it["link"])
        base_text = article_text if len(article_text) >= 250 else (it.get("rss_summary") or it["title"])

        bullets_src = naive_bullets(base_text, max_bullets=BULLETS_PER_ITEM)

        # å…³é”®ï¼šsource=autoï¼Œä¸”æœ‰å¤šå®ä¾‹è½®è¯¢
        title_cn = libre_translate(it["title"], source="auto", target="zh")
        bullets_cn = [libre_translate(b, source="auto", target="zh") for b in bullets_src]

        company = it.get("company", "")
        source = it.get("source", "")

        # ä¸ç”¨é“¾æ¥å½¢å¼ï¼šçº¯æ–‡æœ¬è¾“å‡º
        lines.append(f"{idx}. <b>{esc(title_cn[:180])}</b>")
        lines.append(f"<i>{esc(company)} Â· {esc(source)}</i>")
        for b in bullets_cn:
            b = (b or "").strip()
            if b:
                lines.append(f"â€¢ {esc(b)}")
        lines.append("")

        enriched.append(
            {
                "title_cn": title_cn,
                "bullets_cn": bullets_cn,
                "company": company,
                "source": source,
                "link": it["link"],
            }
        )

    lines.append("â€”")
    lines.append("<i>è¯´æ˜ï¼šä¸ºåˆè§„ä¸ç¨³å®šæ€§ï¼Œæ¨é€ä¸ºâ€œä¸­æ–‡æ‘˜è¦/è¦ç‚¹å¤è¿°â€ï¼Œä¸ç›´æ¥è½¬å‘åŸæ–‡å…¨æ–‡ã€‚</i>")
    return "\n".join(lines).strip(), enriched


def main():
    token = os.getenv("BOT_TOKEN", "").strip()
    chat_id = os.getenv("CHAT_ID", "").strip()
    if not token or not chat_id:
        raise RuntimeError("Missing BOT_TOKEN or CHAT_ID (use GitHub Secrets).")

    items = fetch_news()
    if not items:
        tg_send_message(token, chat_id, "<b>ğŸ§¬ åŒ»è¯æ–°é—»</b>\n\nä»Šå¤©æœªæŠ“åˆ°è¦é—»ã€‚")
        return

    digest, enriched = build_cn_digest(items)
    tg_send_message(token, chat_id, digest)

    # å¯é€‰ï¼šç»™å‰ 3 æ¡å°è¯•é…å›¾ï¼ˆå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰
    for it in enriched[:3]:
        img = try_get_og_image(it["link"])
        if not img:
            continue
        caption = f"ğŸ–¼ï¸ {it['title_cn'][:180]}"
        tg_send_photo(token, chat_id, img, caption)


if __name__ == "__main__":
    main()
