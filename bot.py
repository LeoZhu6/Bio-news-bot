import os
import re
import html
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus

import requests
import feedparser
from bs4 import BeautifulSoup


COMPANIES = [
    ("Pfizer è¾‰ç‘", ("Pfizer", "è¾‰ç‘")),
    ("Merck é»˜æ²™ä¸œ", ("Merck", "é»˜æ²™ä¸œ", "MSD")),
    ("J&J å¼ºç”Ÿ", ("Johnson & Johnson", "J&J", "å¼ºç”Ÿ")),
    ("Roche ç½—æ°", ("Roche", "ç½—æ°")),
    ("Novartis è¯ºå", ("Novartis", "è¯ºå")),
    ("AstraZeneca é˜¿æ–¯åˆ©åº·", ("AstraZeneca", "é˜¿æ–¯åˆ©åº·")),
    ("GSK è‘›å…°ç´ å²å…‹", ("GSK", "GlaxoSmithKline", "è‘›å…°ç´ å²å…‹")),
    ("Sanofi èµ›è¯ºè²", ("Sanofi", "èµ›è¯ºè²")),
    ("BMS ç™¾æ—¶ç¾æ–½è´µå®", ("Bristol Myers Squibb", "BMS", "ç™¾æ—¶ç¾æ–½è´µå®")),
    ("AbbVie è‰¾ä¼¯ç»´", ("AbbVie", "è‰¾ä¼¯ç»´")),
    ("Amgen å®‰è¿›", ("Amgen", "å®‰è¿›")),
    ("Eli Lilly ç¤¼æ¥", ("Eli Lilly", "Lilly", "ç¤¼æ¥")),
    ("Novo Nordisk è¯ºå’Œè¯ºå¾·", ("Novo Nordisk", "è¯ºå’Œè¯ºå¾·")),
    ("Moderna", ("Moderna",)),
    ("BioNTech", ("BioNTech",)),
    ("æ’ç‘åŒ»è¯", ("æ’ç‘åŒ»è¯", "Hengrui")),
    ("ç™¾æµç¥å·", ("ç™¾æµç¥å·", "BeiGene")),
    ("è¯æ˜åº·å¾·", ("è¯æ˜åº·å¾·", "WuXi AppTec")),
    ("å¤æ˜ŸåŒ»è¯", ("å¤æ˜ŸåŒ»è¯", "Fosun Pharma")),
    ("ä¿¡è¾¾ç”Ÿç‰©", ("ä¿¡è¾¾ç”Ÿç‰©", "Innovent")),
    ("å›å®ç”Ÿç‰©", ("å›å®ç”Ÿç‰©", "Junshi Biosciences")),
]

EXTRA_KEYWORDS_DEFAULT = ["FDA", "EMA", "NMPA", "clinical trial", "Phase 3", "acquisition", "approval"]

MAX_ITEMS = int(os.getenv("MAX_ITEMS", "10"))
DAYS_LOOKBACK = int(os.getenv("DAYS_LOOKBACK", "2"))


def google_news_rss_url(query: str) -> str:
    # Google News RSS
    # hl/gl/ceid è¿™é‡Œç”¨ US è‹±æ–‡èšåˆï¼Œè¦†ç›–å›½é™…åª’ä½“ï¼›ä¸­æ–‡å…¬å¸åä¹Ÿèƒ½æœåˆ°
    q = quote_plus(query)
    return f"https://news.google.com/rss/search?q={q}&hl=en-US&gl=US&ceid=US:en"


def clean_html_to_text(s: str) -> str:
    soup = BeautifulSoup(s or "", "lxml")
    text = soup.get_text(" ", strip=True)
    return re.sub(r"\s+", " ", text).strip()


def parse_entry_time(entry):
    tm = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not tm:
        return None
    try:
        return datetime(*tm[:6], tzinfo=timezone.utc)
    except Exception:
        return None


def try_get_og_image(url: str, timeout: float = 10.0):
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "lxml")
        for attrs in (
            {"property": "og:image"},
            {"name": "og:image"},
            {"property": "twitter:image"},
            {"name": "twitter:image"},
        ):
            tag = soup.find("meta", attrs=attrs)
            if tag and tag.get("content"):
                img = tag["content"].strip()
                if img.startswith("http"):
                    return img
    except Exception:
        return None
    return None


def esc(x: str) -> str:
    return html.escape(x or "")


def format_digest(items):
    lines = []
    lines.append("<b>ğŸ§¬ åŒ»è¯å¤§å‚æ–°é—»é€Ÿé€’ï¼ˆå›½å†…å¤–ï¼‰</b>")
    lines.append(f"<i>{esc(datetime.now().strftime('%Y-%m-%d %H:%M'))}</i>")
    lines.append("")
    for i, it in enumerate(items, 1):
        title = esc((it.get("title") or "")[:200])
        link = it.get("link") or ""
        company = esc(it.get("company") or "")
        source = esc(it.get("source") or "")
        summary = esc((it.get("summary") or "")[:260])

        headline = f'ğŸ”¹ <a href="{esc(link)}">{title}</a>' if link else f"ğŸ”¹ {title}"
        meta = " Â· ".join([x for x in [company, source] if x])
        if meta:
            meta = f"<i>{meta}</i>"

        lines.append(f"{i}. {headline}")
        if meta:
            lines.append(meta)
        if summary:
            lines.append(summary)
        lines.append("")
    lines.append("â€”")
    lines.append("<i>æ¥æºï¼šGoogle News RSS èšåˆï¼›é…å›¾ä¸ºç½‘é¡µ OG å›¾ï¼ˆå¯èƒ½å› ç«™ç‚¹é™åˆ¶ç¼ºå¤±ï¼‰ã€‚</i>")
    return "\n".join(lines).strip()


def tg_send_message(token: str, chat_id: str, text: str):
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    r = requests.post(
        url,
        timeout=15,
        data={
            "chat_id": chat_id,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": "true",
        },
    )
    r.raise_for_status()


def tg_send_photo(token: str, chat_id: str, photo_url: str, caption: str):
    url = f"https://api.telegram.org/bot{token}/sendPhoto"
    r = requests.post(
        url,
        timeout=20,
        data={"chat_id": chat_id, "caption": caption, "parse_mode": "HTML"},
        files=None,
        params={"photo": photo_url},
    )
    # æœ‰äº› host ä¼šæ‹’ç» Telegram æ‹‰å›¾ï¼Œå¤±è´¥å°±å¿½ç•¥
    if r.status_code >= 400:
        return
    return


def fetch_news():
    extra_keywords = [x.strip() for x in os.getenv("EXTRA_KEYWORDS", "").split(",") if x.strip()]
    if not extra_keywords:
        extra_keywords = EXTRA_KEYWORDS_DEFAULT

    cutoff = datetime.now(timezone.utc) - timedelta(days=DAYS_LOOKBACK)
    all_items = []
    seen_links = set()

    for company_name, queries in COMPANIES:
        base = "(" + " OR ".join([f'"{q}"' if " " in q else q for q in queries]) + ")"
        extra = "(" + " OR ".join([f'"{k}"' if " " in k else k for k in extra_keywords]) + ")"
        q = f"{base} {extra}"

        feed = feedparser.parse(google_news_rss_url(q))
        for e in feed.entries:
            title = (e.get("title") or "").strip()
            link = (e.get("link") or "").strip()
            if not title or not link or link in seen_links:
                continue

            published = parse_entry_time(e)
            if published and published < cutoff:
                continue

            source = ""
            if e.get("source") and isinstance(e["source"], dict):
                source = (e["source"].get("title") or "").strip()

            summary = clean_html_to_text(e.get("summary", "") or e.get("description", "") or "")

            all_items.append(
                {
                    "company": company_name,
                    "title": title,
                    "link": link,
                    "source": source,
                    "published": published.isoformat() if published else "",
                    "summary": summary,
                }
            )
            seen_links.add(link)

    all_items.sort(key=lambda x: x.get("published") or "", reverse=True)
    return all_items[:MAX_ITEMS]


def main():
    token = os.getenv("BOT_TOKEN", "").strip()
    chat_id = os.getenv("CHAT_ID", "").strip()
    if not token or not chat_id:
        raise RuntimeError("Missing BOT_TOKEN or CHAT_ID (use GitHub Secrets).")

    items = fetch_news()
    if not items:
        tg_send_message(token, chat_id, "<b>ğŸ§¬ åŒ»è¯æ–°é—»</b>\n\nä»Šå¤©æœªæŠ“åˆ°è¦é—»ã€‚")
        return

    tg_send_message(token, chat_id, format_digest(items))

    # å°è¯•ç»™å‰ 3 æ¡è¡¥å›¾
    for it in items[:3]:
        img = try_get_og_image(it["link"])
        if not img:
            continue
        caption = f'ğŸ–¼ï¸ <a href="{esc(it["link"])}">{esc(it["title"][:180])}</a>'
        tg_send_photo(token, chat_id, img, caption)


if __name__ == "__main__":
    main()
